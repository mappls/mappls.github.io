<!doctype html>
<title>Project: learnbet - mappls</title>
<link rel="stylesheet" href="/static/style.css">
<nav>
    <a href="/projects">
        <img id="header-img" src="/static/my_img1.jpg">
    </a>
    <h1 id="header-text">mpav</h1>
    <ul>
        <li><a class="action" href="/">Home</a></li>
        <li><a class="action" href="/projects">Projects</a></li>
        <li><a class="action" href="/research">Research</a></li>
    </ul>
</nav>
<div>
    <hr id="header-line">
</div>
<section class="content">
    <header>
        
<h1>Project: learnbet</h1>

    </header>
    
<article class="post">
    <header>
        <div class="special">
            An end-to-end Machine Learning project, for forecasting outcomes of football matches. It includes the stages of: data collection, cleaning, storage, processing, predictive modeling, model evaluation, statistical analysis and production deployment. 

This page describes an older version of the project, while a new one is currently in development.

&lt;hr&gt;

I initially started this project in 2016 with the aim to learn Machine Learning, and all the other steps that come before and after it. Therefore, most of the code is built &#34;from scratch&#34; i.e., I didn&#39;t use any ML frameworks, or data preprocessing, or statistical packages, but tried to code the needed bits in Python. For example, I created this small [NeuralNet](https://github.com/Misko07/NeuralNet) project to train a neural network. The forward and back-propagation functions, gradient descent and the rest are coded using the instructions in the famous [Machine Learning course](https://www.coursera.org/learn/machine-learning) by Andrew Ng. Of course, results were really sub-optimal, but the efforts paid off because of the things learned!

## Data ingestion and storage

Using available datasets on the Internet is not fun. I started scraping football data off a few websites back in 2016, and by now there&#39;s quite a lot. It&#39;s mainly data on &#34;matches&#34; - goals scored / allowed, statistics, and different betting odds. 

I use Python for the web scraping with a headless Chrome browser, and BeautifulSoup for processing the HTML. Scripts scraping different websites are started as separate processes, which get activated in every ~1 hour. This is active 24/7. As new matches arrive, data is initially cleaned up, and saved in MongoDB.

## Data preprocessing

In this old Learnbet version there was no data pipe to extract transform and load from MongoDB to the ML model, but it would&#39;ve been handy. For each separate model, I had to manually create a new Python script to gather data from the DB, apply the needed functions, and output a matrix-like data structure for the model&#39;s inputs and targets. 

## Modeling

As mentioned, the modeling part was basically done from scratch, using this small [NeuralNet](https://github.com/Misko07/NeuralNet). So is the model evaluation. One of the first models I used was predicting a 1 / x / 2 outcome (class) of a match. Then different classes had different accuracy / precision / recall. The following figure shows the distribution of match outcomes and predictions, where predictions around 0 are &#34;home win&#34;, 0.5 - &#34;draw&#34;, and 1 - &#34;away win&#34;. 

![first_one](../static/first_one_hist.png)

Later on, after being comfortable with using my neural networks, I continued experimenting with models of the `scikit-learn` library, such as: support vector machines (SVMs), logistic regression, decision trees and random forests.

## Deploying in production
The first few models were built to work as &#34;console&#34; apps. You pass the names of the two playing teams (and an optional date), and get as output the prediction. Later on, I setup a small Flask app access predictions (and other data) on the web, as in the next figure. The web app was first set on GCP, but later on switched to AWS. 

![odds_web](../static/odds_web.png)


## Reporting, logging, statistics
As my first end-to-end ML project, I had to monitor the app for things like scrapping crashes and data inconsistencies. Here the `logging` library of Python came very useful, as all important events across the pipeline were gathered in a single text-based log file. 

Monitoring performance is always a good idea. In Learnbet I automated the creation of a few separate reports: model stats (all-time or last `n` matches), league stats, and team stats.

 

        </div>
    </header>
    <div>
    </div>
    <p class="body">An end-to-end Machine Learning project, for forecasting outcomes of football matches. It includes the stages of: data collection, cleaning, storage, processing, predictive modeling, model evaluation, statistical analysis and production deployment. 

There are two versions of this project:

- Old version - created mainly for learning purposes, built many tools from scratch;
- New version (under construction) - builds up on the gathered data, and will reconstruct the predictive modeling stage.</p>
</article>

</section>